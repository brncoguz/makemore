{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the text file\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set of bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12f0f3670>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAACHCAYAAABK4hAcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN2klEQVR4nO3df2hV9ePH8dfd2q4/urs6137cNufUUmpukrolkgkbTgvJ9A8r/1hDjOoqzlHJAl1CsDAIqSQjKP/xV0ImyQdDlpsE8wcTMaH21SFfr8xtKR/vdOZcu+/PH3263+9Nnd7tvXt2r88HHLj33Df3vHjzlr0899x7XMYYIwAAAAuSnA4AAAASB8UCAABYQ7EAAADWUCwAAIA1FAsAAGANxQIAAFhDsQAAANY8EsuDhUIhtbe3y+PxyOVyxfLQAABgkIwxun79unw+n5KSBj4nEdNi0d7erry8vFgeEgAAWBIIBJSbmzvgmJgWC4/HI0n631OTlPbo0D6FefnJGTYiAQCA+/hTffpZ/wr/HR9ITIvF3x9/pD2apDTP0IrFI64UG5EAAMD9/PfmHw9yGQMXbwIAAGsoFgAAwBqKBQAAsGZQxWLbtm2aNGmSRo0apdLSUp04ccJ2LgAAEIeiLhZ79+5VTU2N6urqdOrUKRUXF6uiokJdXV3DkQ8AAMSRqIvFJ598otWrV6uqqkpPPfWUtm/frjFjxujrr78ejnwAACCORFUsbt++rZaWFpWXl//fGyQlqby8XM3NzXeM7+3tVXd3d8QGAAASV1TF4sqVK+rv71dWVlbE/qysLHV0dNwxvr6+Xl6vN7zxq5sAACS2Yf1WSG1trYLBYHgLBALDeTgAAOCwqH55MyMjQ8nJyers7IzY39nZqezs7DvGu91uud3uoSUEAABxI6ozFqmpqZo1a5YaGhrC+0KhkBoaGjR37lzr4QAAQHyJ+l4hNTU1qqys1OzZs1VSUqKtW7eqp6dHVVVVw5EPAADEkaiLxYoVK/T7779r06ZN6ujo0MyZM3Xo0KE7LugEAAAPH5cxxsTqYN3d3fJ6vfr3/0we8t1NK3wz7YQCAAAD+tP0qVEHFAwGlZaWNuBY7hUCAACsifqjEBtefnKGHnGlOHHoh86P7aetvA9niAAAD4IzFgAAwBqKBQAAsIZiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwBqKBQAAsIZiAQAArKFYAAAAaygWAADAGooFAACw5hGnA2B4VfhmOh0BCeLH9tNW3oc1CSQ2zlgAAABrKBYAAMAaigUAALCGYgEAAKyJqljU19drzpw58ng8yszM1NKlS9Xa2jpc2QAAQJyJqlg0NTXJ7/fr2LFjOnz4sPr6+rRw4UL19PQMVz4AABBHovq66aFDhyKe79ixQ5mZmWppadH8+fOtBgMAAPFnSL9jEQwGJUnp6el3fb23t1e9vb3h593d3UM5HAAAGOEGffFmKBRSdXW15s2bp8LCwruOqa+vl9frDW95eXmDDgoAAEa+QRcLv9+vs2fPas+ePfccU1tbq2AwGN4CgcBgDwcAAOLAoD4KWbNmjQ4ePKijR48qNzf3nuPcbrfcbvegwwEAgPgSVbEwxmjt2rXav3+/GhsbVVBQMFy5AABAHIqqWPj9fu3atUsHDhyQx+NRR0eHJMnr9Wr06NHDEhAAAMSPqK6x+OKLLxQMBrVgwQLl5OSEt7179w5XPgAAEEei/igEAADgXrhXCAAAsIZiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwBqKBQAAsIZiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALDmEacDDNaP7aetvVeFb6a19wISFf9OADwIzlgAAABrKBYAAMAaigUAALCGYgEAAKwZUrH46KOP5HK5VF1dbSkOAACIZ4MuFidPntSXX36poqIim3kAAEAcG1SxuHHjhlauXKmvvvpK48ePt50JAADEqUEVC7/frxdffFHl5eUDjuvt7VV3d3fEBgAAElfUP5C1Z88enTp1SidPnrzv2Pr6em3evHlQwQAAQPyJ6oxFIBDQunXrtHPnTo0aNeq+42traxUMBsNbIBAYdFAAADDyRXXGoqWlRV1dXXrmmWfC+/r7+3X06FF9/vnn6u3tVXJycvg1t9stt9ttLy0AABjRoioWZWVl+uWXXyL2VVVVafr06dqwYUNEqQAAAA+fqIqFx+NRYWFhxL6xY8dqwoQJd+wHAAAPH355EwAAWDPk26Y3NjZaiAEAABIBZywAAIA1Qz5jEQ1jjCTpT/VJZmjv1X09ZCHRX/40fdbeCwCARPOn/vo7+fff8YG4zIOMsuTSpUvKy8uL1eEAAIBFgUBAubm5A46JabEIhUJqb2+Xx+ORy+W657ju7m7l5eUpEAgoLS0tVvEeWsx37DDXscV8xxbzHVuxnG9jjK5fvy6fz6ekpIGvoojpRyFJSUn3bTr/X1paGoszhpjv2GGuY4v5ji3mO7ZiNd9er/eBxnHxJgAAsIZiAQAArBmRxcLtdquuro77jMQI8x07zHVsMd+xxXzH1kid75hevAkAABLbiDxjAQAA4hPFAgAAWEOxAAAA1lAsAACANRQLAABgzYgrFtu2bdOkSZM0atQolZaW6sSJE05HSkgffPCBXC5XxDZ9+nSnYyWMo0ePasmSJfL5fHK5XPr+++8jXjfGaNOmTcrJydHo0aNVXl6uc+fOORM2Adxvvl9//fU71vuiRYucCRvn6uvrNWfOHHk8HmVmZmrp0qVqbW2NGHPr1i35/X5NmDBBjz76qJYvX67Ozk6HEse3B5nvBQsW3LG+33zzTYcSj7BisXfvXtXU1Kiurk6nTp1ScXGxKioq1NXV5XS0hPT000/r8uXL4e3nn392OlLC6OnpUXFxsbZt23bX17ds2aJPP/1U27dv1/HjxzV27FhVVFTo1q1bMU6aGO4335K0aNGiiPW+e/fuGCZMHE1NTfL7/Tp27JgOHz6svr4+LVy4UD09PeEx69ev1w8//KB9+/apqalJ7e3tWrZsmYOp49eDzLckrV69OmJ9b9myxaHEkswIUlJSYvx+f/h5f3+/8fl8pr6+3sFUiamurs4UFxc7HeOhIMns378//DwUCpns7Gzz8ccfh/ddu3bNuN1us3v3bgcSJpZ/zrcxxlRWVpqXXnrJkTyJrqury0gyTU1Nxpi/1nJKSorZt29feMyvv/5qJJnm5manYiaMf863McY8//zzZt26dc6F+ocRc8bi9u3bamlpUXl5eXhfUlKSysvL1dzc7GCyxHXu3Dn5fD5NnjxZK1eu1MWLF52O9FC4cOGCOjo6Ita61+tVaWkpa30YNTY2KjMzU9OmTdNbb72lq1evOh0pIQSDQUlSenq6JKmlpUV9fX0R63v69OmaOHEi69uCf87333bu3KmMjAwVFhaqtrZWN2/edCKepBjf3XQgV65cUX9/v7KysiL2Z2Vl6bfffnMoVeIqLS3Vjh07NG3aNF2+fFmbN2/Wc889p7Nnz8rj8TgdL6F1dHRI0l3X+t+vwa5FixZp2bJlKigoUFtbm95//30tXrxYzc3NSk5Odjpe3AqFQqqurta8efNUWFgo6a/1nZqaqnHjxkWMZX0P3d3mW5Jee+015efny+fz6cyZM9qwYYNaW1v13XffOZJzxBQLxNbixYvDj4uKilRaWqr8/Hx9++23WrVqlYPJAPteeeWV8OMZM2aoqKhIU6ZMUWNjo8rKyhxMFt/8fr/Onj3L9Vkxcq/5fuONN8KPZ8yYoZycHJWVlamtrU1TpkyJdcyRc/FmRkaGkpOT77hyuLOzU9nZ2Q6leniMGzdOTz75pM6fP+90lIT393pmrTtn8uTJysjIYL0PwZo1a3Tw4EEdOXJEubm54f3Z2dm6ffu2rl27FjGe9T0095rvuyktLZUkx9b3iCkWqampmjVrlhoaGsL7QqGQGhoaNHfuXAeTPRxu3LihtrY25eTkOB0l4RUUFCg7OztirXd3d+v48eOs9Ri5dOmSrl69ynofBGOM1qxZo/379+unn35SQUFBxOuzZs1SSkpKxPpubW3VxYsXWd+DcL/5vpvTp09LkmPre0R9FFJTU6PKykrNnj1bJSUl2rp1q3p6elRVVeV0tITzzjvvaMmSJcrPz1d7e7vq6uqUnJysV1991eloCeHGjRsR/1u4cOGCTp8+rfT0dE2cOFHV1dX68MMP9cQTT6igoEAbN26Uz+fT0qVLnQsdxwaa7/T0dG3evFnLly9Xdna22tra9N5772nq1KmqqKhwMHV88vv92rVrlw4cOCCPxxO+bsLr9Wr06NHyer1atWqVampqlJ6errS0NK1du1Zz587Vs88+63D6+HO/+W5ra9OuXbv0wgsvaMKECTpz5ozWr1+v+fPnq6ioyJnQTn8t5Z8+++wzM3HiRJOammpKSkrMsWPHnI6UkFasWGFycnJMamqqefzxx82KFSvM+fPnnY6VMI4cOWIk3bFVVlYaY/76yunGjRtNVlaWcbvdpqyszLS2tjobOo4NNN83b940CxcuNI899phJSUkx+fn5ZvXq1aajo8Pp2HHpbvMsyXzzzTfhMX/88Yd5++23zfjx482YMWPMyy+/bC5fvuxc6Dh2v/m+ePGimT9/vklPTzdut9tMnTrVvPvuuyYYDDqW2fXf4AAAAEM2Yq6xAAAA8Y9iAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGv+A6sEjbDe9GoiAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float() # otherwise its int64\n",
    "print(xenc.dtype)\n",
    "plt.imshow(xenc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
       "          0.0791,  0.9046, -0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,\n",
       "          1.5618, -1.6261,  0.6772, -0.8404,  0.9849, -0.1484, -1.4795,  0.4483,\n",
       "         -0.0707,  2.4968,  2.4448]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 2147483647\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # weight matrix for 27 inputs X 27 neurons\n",
    "xenc[:1] @ W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7940,  0.7888,  0.9730,  0.3326,  1.3309,  0.9708,  0.2129,  1.8311,\n",
       "          1.0824,  2.4710,  0.6242,  2.1964,  0.7200,  0.6486,  3.9469, 18.7908,\n",
       "          4.7673,  0.1967,  1.9683,  0.4315,  2.6775,  0.8621,  0.2277,  1.5656,\n",
       "          0.9317, 12.1434, 11.5281]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc[:1] @ W).exp() # negative numbers go between 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7590, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # otherwise its int64\n",
    "logits = xenc @ W  # log-counts\n",
    "counts = logits.exp() # like a count matrix\n",
    "probs = counts / counts.sum(1, keepdims=True) # exponantiate + normalise = SOFTMAX\n",
    "loss = -probs[torch.arange(len(probs)), ys].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset grad / backward pass\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the parameters\n",
    "W.data -= 0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th iteration loss 3.76770281791687\n",
      "1th iteration loss 3.3782901763916016\n",
      "2th iteration loss 3.160780668258667\n",
      "3th iteration loss 3.0269784927368164\n",
      "4th iteration loss 2.934335470199585\n",
      "5th iteration loss 2.8671212196350098\n",
      "6th iteration loss 2.8165693283081055\n",
      "7th iteration loss 2.777078866958618\n",
      "8th iteration loss 2.7451984882354736\n",
      "9th iteration loss 2.7187840938568115\n",
      "10th iteration loss 2.6964662075042725\n",
      "11th iteration loss 2.677338123321533\n",
      "12th iteration loss 2.660775661468506\n",
      "13th iteration loss 2.6463255882263184\n",
      "14th iteration loss 2.6336419582366943\n",
      "15th iteration loss 2.6224513053894043\n",
      "16th iteration loss 2.6125295162200928\n",
      "17th iteration loss 2.6036906242370605\n",
      "18th iteration loss 2.595780372619629\n",
      "19th iteration loss 2.58866810798645\n",
      "20th iteration loss 2.582244396209717\n",
      "21th iteration loss 2.576418876647949\n",
      "22th iteration loss 2.5711135864257812\n",
      "23th iteration loss 2.5662636756896973\n",
      "24th iteration loss 2.561814308166504\n",
      "25th iteration loss 2.5577187538146973\n",
      "26th iteration loss 2.5539374351501465\n",
      "27th iteration loss 2.550436019897461\n",
      "28th iteration loss 2.5471866130828857\n",
      "29th iteration loss 2.5441641807556152\n",
      "30th iteration loss 2.541347026824951\n",
      "31th iteration loss 2.538717031478882\n",
      "32th iteration loss 2.536257266998291\n",
      "33th iteration loss 2.533953905105591\n",
      "34th iteration loss 2.5317933559417725\n",
      "35th iteration loss 2.529764413833618\n",
      "36th iteration loss 2.5278565883636475\n",
      "37th iteration loss 2.5260603427886963\n",
      "38th iteration loss 2.524367570877075\n",
      "39th iteration loss 2.5227701663970947\n",
      "40th iteration loss 2.521261215209961\n",
      "41th iteration loss 2.519834041595459\n",
      "42th iteration loss 2.5184831619262695\n",
      "43th iteration loss 2.517202854156494\n",
      "44th iteration loss 2.515988349914551\n",
      "45th iteration loss 2.5148351192474365\n",
      "46th iteration loss 2.5137391090393066\n",
      "47th iteration loss 2.5126960277557373\n",
      "48th iteration loss 2.511702537536621\n",
      "49th iteration loss 2.510756254196167\n",
      "50th iteration loss 2.5098531246185303\n",
      "51th iteration loss 2.50899076461792\n",
      "52th iteration loss 2.508167028427124\n",
      "53th iteration loss 2.5073790550231934\n",
      "54th iteration loss 2.506624460220337\n",
      "55th iteration loss 2.50590181350708\n",
      "56th iteration loss 2.50520920753479\n",
      "57th iteration loss 2.504544734954834\n",
      "58th iteration loss 2.503906726837158\n",
      "59th iteration loss 2.5032942295074463\n",
      "60th iteration loss 2.5027053356170654\n",
      "61th iteration loss 2.502138614654541\n",
      "62th iteration loss 2.501593589782715\n",
      "63th iteration loss 2.501068353652954\n",
      "64th iteration loss 2.5005624294281006\n",
      "65th iteration loss 2.5000743865966797\n",
      "66th iteration loss 2.4996039867401123\n",
      "67th iteration loss 2.499149799346924\n",
      "68th iteration loss 2.498711109161377\n",
      "69th iteration loss 2.4982876777648926\n",
      "70th iteration loss 2.497878074645996\n",
      "71th iteration loss 2.4974820613861084\n",
      "72th iteration loss 2.4970993995666504\n",
      "73th iteration loss 2.4967284202575684\n",
      "74th iteration loss 2.4963696002960205\n",
      "75th iteration loss 2.4960222244262695\n",
      "76th iteration loss 2.495685577392578\n",
      "77th iteration loss 2.495358943939209\n",
      "78th iteration loss 2.495042562484741\n",
      "79th iteration loss 2.4947359561920166\n",
      "80th iteration loss 2.4944381713867188\n",
      "81th iteration loss 2.4941489696502686\n",
      "82th iteration loss 2.493868589401245\n",
      "83th iteration loss 2.4935965538024902\n",
      "84th iteration loss 2.4933319091796875\n",
      "85th iteration loss 2.493074655532837\n",
      "86th iteration loss 2.4928245544433594\n",
      "87th iteration loss 2.492581844329834\n",
      "88th iteration loss 2.4923460483551025\n",
      "89th iteration loss 2.4921162128448486\n",
      "90th iteration loss 2.4918928146362305\n",
      "91th iteration loss 2.4916751384735107\n",
      "92th iteration loss 2.4914638996124268\n",
      "93th iteration loss 2.491258382797241\n",
      "94th iteration loss 2.4910576343536377\n",
      "95th iteration loss 2.4908623695373535\n",
      "96th iteration loss 2.4906721115112305\n",
      "97th iteration loss 2.4904863834381104\n",
      "98th iteration loss 2.4903061389923096\n",
      "99th iteration loss 2.4901299476623535\n",
      "100th iteration loss 2.4899585247039795\n",
      "101th iteration loss 2.4897913932800293\n",
      "102th iteration loss 2.4896280765533447\n",
      "103th iteration loss 2.489469289779663\n",
      "104th iteration loss 2.489314079284668\n",
      "105th iteration loss 2.4891624450683594\n",
      "106th iteration loss 2.4890146255493164\n",
      "107th iteration loss 2.48887038230896\n",
      "108th iteration loss 2.488729476928711\n",
      "109th iteration loss 2.4885921478271484\n",
      "110th iteration loss 2.488457441329956\n",
      "111th iteration loss 2.4883265495300293\n",
      "112th iteration loss 2.4881982803344727\n",
      "113th iteration loss 2.4880733489990234\n",
      "114th iteration loss 2.4879512786865234\n",
      "115th iteration loss 2.4878311157226562\n",
      "116th iteration loss 2.4877145290374756\n",
      "117th iteration loss 2.487600564956665\n",
      "118th iteration loss 2.4874887466430664\n",
      "119th iteration loss 2.487379550933838\n",
      "120th iteration loss 2.4872727394104004\n",
      "121th iteration loss 2.487168312072754\n",
      "122th iteration loss 2.4870660305023193\n",
      "123th iteration loss 2.486966133117676\n",
      "124th iteration loss 2.486868381500244\n",
      "125th iteration loss 2.4867725372314453\n",
      "126th iteration loss 2.4866786003112793\n",
      "127th iteration loss 2.4865870475769043\n",
      "128th iteration loss 2.486496925354004\n",
      "129th iteration loss 2.4864094257354736\n",
      "130th iteration loss 2.486323118209839\n",
      "131th iteration loss 2.486238718032837\n",
      "132th iteration loss 2.4861557483673096\n",
      "133th iteration loss 2.4860751628875732\n",
      "134th iteration loss 2.4859955310821533\n",
      "135th iteration loss 2.485917806625366\n",
      "136th iteration loss 2.485841751098633\n",
      "137th iteration loss 2.485766887664795\n",
      "138th iteration loss 2.4856934547424316\n",
      "139th iteration loss 2.485621690750122\n",
      "140th iteration loss 2.485551357269287\n",
      "141th iteration loss 2.4854824542999268\n",
      "142th iteration loss 2.485414743423462\n",
      "143th iteration loss 2.4853484630584717\n",
      "144th iteration loss 2.485283136367798\n",
      "145th iteration loss 2.4852192401885986\n",
      "146th iteration loss 2.485156536102295\n",
      "147th iteration loss 2.4850950241088867\n",
      "148th iteration loss 2.485034465789795\n",
      "149th iteration loss 2.4849750995635986\n",
      "150th iteration loss 2.484916925430298\n",
      "151th iteration loss 2.4848597049713135\n",
      "152th iteration loss 2.4848036766052246\n",
      "153th iteration loss 2.484748363494873\n",
      "154th iteration loss 2.484694242477417\n",
      "155th iteration loss 2.4846408367156982\n",
      "156th iteration loss 2.484588861465454\n",
      "157th iteration loss 2.4845376014709473\n",
      "158th iteration loss 2.484487295150757\n",
      "159th iteration loss 2.4844374656677246\n",
      "160th iteration loss 2.484388828277588\n",
      "161th iteration loss 2.4843411445617676\n",
      "162th iteration loss 2.4842941761016846\n",
      "163th iteration loss 2.484247922897339\n",
      "164th iteration loss 2.4842028617858887\n",
      "165th iteration loss 2.4841578006744385\n",
      "166th iteration loss 2.484113931655884\n",
      "167th iteration loss 2.4840710163116455\n",
      "168th iteration loss 2.4840285778045654\n",
      "169th iteration loss 2.4839866161346436\n",
      "170th iteration loss 2.483945369720459\n",
      "171th iteration loss 2.48390531539917\n",
      "172th iteration loss 2.4838650226593018\n",
      "173th iteration loss 2.483825922012329\n",
      "174th iteration loss 2.4837875366210938\n",
      "175th iteration loss 2.4837493896484375\n",
      "176th iteration loss 2.483712911605835\n",
      "177th iteration loss 2.483675718307495\n",
      "178th iteration loss 2.4836394786834717\n",
      "179th iteration loss 2.4836041927337646\n",
      "180th iteration loss 2.483569383621216\n",
      "181th iteration loss 2.483534574508667\n",
      "182th iteration loss 2.4835007190704346\n",
      "183th iteration loss 2.4834671020507812\n",
      "184th iteration loss 2.483433961868286\n",
      "185th iteration loss 2.4834022521972656\n",
      "186th iteration loss 2.483370065689087\n",
      "187th iteration loss 2.4833385944366455\n",
      "188th iteration loss 2.4833078384399414\n",
      "189th iteration loss 2.4832773208618164\n",
      "190th iteration loss 2.4832472801208496\n",
      "191th iteration loss 2.483217716217041\n",
      "192th iteration loss 2.4831883907318115\n",
      "193th iteration loss 2.4831597805023193\n",
      "194th iteration loss 2.4831314086914062\n",
      "195th iteration loss 2.4831035137176514\n",
      "196th iteration loss 2.4830760955810547\n",
      "197th iteration loss 2.483048677444458\n",
      "198th iteration loss 2.4830219745635986\n",
      "199th iteration loss 2.4829957485198975\n",
      "200th iteration loss 2.4829697608947754\n",
      "201th iteration loss 2.4829440116882324\n",
      "202th iteration loss 2.4829187393188477\n",
      "203th iteration loss 2.482893705368042\n",
      "204th iteration loss 2.4828693866729736\n",
      "205th iteration loss 2.4828450679779053\n",
      "206th iteration loss 2.482821226119995\n",
      "207th iteration loss 2.482797622680664\n",
      "208th iteration loss 2.482774257659912\n",
      "209th iteration loss 2.4827513694763184\n",
      "210th iteration loss 2.4827284812927246\n",
      "211th iteration loss 2.482706308364868\n",
      "212th iteration loss 2.482684373855591\n",
      "213th iteration loss 2.4826626777648926\n",
      "214th iteration loss 2.4826409816741943\n",
      "215th iteration loss 2.4826200008392334\n",
      "216th iteration loss 2.4825987815856934\n",
      "217th iteration loss 2.4825785160064697\n",
      "218th iteration loss 2.482558250427246\n",
      "219th iteration loss 2.4825377464294434\n",
      "220th iteration loss 2.482518196105957\n",
      "221th iteration loss 2.4824984073638916\n",
      "222th iteration loss 2.4824793338775635\n",
      "223th iteration loss 2.4824600219726562\n",
      "224th iteration loss 2.4824414253234863\n",
      "225th iteration loss 2.4824228286743164\n",
      "226th iteration loss 2.4824042320251465\n",
      "227th iteration loss 2.4823861122131348\n",
      "228th iteration loss 2.4823684692382812\n",
      "229th iteration loss 2.4823508262634277\n",
      "230th iteration loss 2.482333183288574\n",
      "231th iteration loss 2.4823157787323\n",
      "232th iteration loss 2.4822990894317627\n",
      "233th iteration loss 2.4822819232940674\n",
      "234th iteration loss 2.4822654724121094\n",
      "235th iteration loss 2.4822492599487305\n",
      "236th iteration loss 2.4822330474853516\n",
      "237th iteration loss 2.4822168350219727\n",
      "238th iteration loss 2.482201099395752\n",
      "239th iteration loss 2.4821853637695312\n",
      "240th iteration loss 2.4821698665618896\n",
      "241th iteration loss 2.482154607772827\n",
      "242th iteration loss 2.4821393489837646\n",
      "243th iteration loss 2.4821245670318604\n",
      "244th iteration loss 2.482109785079956\n",
      "245th iteration loss 2.4820950031280518\n",
      "246th iteration loss 2.4820809364318848\n",
      "247th iteration loss 2.4820666313171387\n",
      "248th iteration loss 2.4820523262023926\n",
      "249th iteration loss 2.482038736343384\n",
      "250th iteration loss 2.482025146484375\n",
      "251th iteration loss 2.482011556625366\n",
      "252th iteration loss 2.4819979667663574\n",
      "253th iteration loss 2.4819846153259277\n",
      "254th iteration loss 2.481971502304077\n",
      "255th iteration loss 2.4819588661193848\n",
      "256th iteration loss 2.481945753097534\n",
      "257th iteration loss 2.481933116912842\n",
      "258th iteration loss 2.4819202423095703\n",
      "259th iteration loss 2.481908082962036\n",
      "260th iteration loss 2.481895923614502\n",
      "261th iteration loss 2.4818837642669678\n",
      "262th iteration loss 2.4818718433380127\n",
      "263th iteration loss 2.4818601608276367\n",
      "264th iteration loss 2.4818480014801025\n",
      "265th iteration loss 2.4818363189697266\n",
      "266th iteration loss 2.481825113296509\n",
      "267th iteration loss 2.481813669204712\n",
      "268th iteration loss 2.4818027019500732\n",
      "269th iteration loss 2.4817914962768555\n",
      "270th iteration loss 2.481780529022217\n",
      "271th iteration loss 2.481769561767578\n",
      "272th iteration loss 2.4817588329315186\n",
      "273th iteration loss 2.481748104095459\n",
      "274th iteration loss 2.4817376136779785\n",
      "275th iteration loss 2.481727361679077\n",
      "276th iteration loss 2.4817168712615967\n",
      "277th iteration loss 2.4817068576812744\n",
      "278th iteration loss 2.481696844100952\n",
      "279th iteration loss 2.48168683052063\n",
      "280th iteration loss 2.4816770553588867\n",
      "281th iteration loss 2.4816672801971436\n",
      "282th iteration loss 2.4816575050354004\n",
      "283th iteration loss 2.4816477298736572\n",
      "284th iteration loss 2.481638193130493\n",
      "285th iteration loss 2.4816291332244873\n",
      "286th iteration loss 2.4816195964813232\n",
      "287th iteration loss 2.4816105365753174\n",
      "288th iteration loss 2.4816012382507324\n",
      "289th iteration loss 2.4815924167633057\n",
      "290th iteration loss 2.481583595275879\n",
      "291th iteration loss 2.4815750122070312\n",
      "292th iteration loss 2.4815661907196045\n",
      "293th iteration loss 2.481557607650757\n",
      "294th iteration loss 2.481549024581909\n",
      "295th iteration loss 2.4815404415130615\n",
      "296th iteration loss 2.481531858444214\n",
      "297th iteration loss 2.4815237522125244\n",
      "298th iteration loss 2.481515645980835\n",
      "299th iteration loss 2.4815075397491455\n"
     ]
    }
   ],
   "source": [
    "lr = 50\n",
    "regularization_const = 0.01\n",
    "for i in range(300):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(probs)), ys].log().mean() + regularization_const * (W**2).mean() # this will push W to be 0\n",
    "    print(f'{i}th iteration loss {loss}')\n",
    "\n",
    "    # reset grad / backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update the parameters\n",
    "    W.data -= lr * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze\n",
      "momasurailezityha\n",
      "konimittain\n",
      "llayn\n",
      "ka\n",
      "da\n",
      "staiyaubrtthrigotai\n",
      "moliellavo\n",
      "ke\n",
      "teda\n",
      "ka\n",
      "emimmsade\n",
      "enkaviyny\n",
      "fobspehinivenvtahlasu\n",
      "dsor\n",
      "br\n",
      "jol\n",
      "pen\n",
      "aisan\n",
      "ja\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "\n",
    "# annnnd we get the exact same results as the first demo :) \n",
    "seed = 2147483647\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "# sample 20 names\n",
    "ix = 0\n",
    "for _ in range(20):\n",
    "    out = ''\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out += itos[ix]\n",
    "    print(out)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
